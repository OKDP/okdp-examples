{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# üß≠ NYC Yellow Taxi Data Analysis with Spark (PySpark)\n",
    "\n",
    "This notebook reads the **NYC Yellow Taxi dataset** directly from **MinIO/S3 (s3a://)** in **Parquet** format and performs sampling + EDA with **Spark**.\n",
    "\n",
    "Dataset path used:\n",
    "- `s3a://okdp/examples/data/raw/tripdata/yellow/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "readme",
   "metadata": {},
   "source": [
    "## üìò Notebook Documentation (README)\n",
    "\n",
    "### What this notebook does\n",
    "This notebook demonstrates a practical Spark-based EDA workflow on the NYC Yellow Taxi dataset stored in **Parquet** on **S3-compatible storage (MinIO)**.\n",
    "\n",
    "You will:\n",
    "- Configure Spark to access MinIO via `s3a://`\n",
    "- Load Parquet into a Spark DataFrame\n",
    "- Inspect schema with both DataFrame and SQL approaches\n",
    "- Build a **balanced per-day random sample** (to reduce temporal skew)\n",
    "- Engineer time features (hour of day, day of week)\n",
    "- Run simple data quality checks\n",
    "- Perform aggregations and visualize results with **Altair** (via small Pandas extracts)\n",
    "\n",
    "### Why balanced sampling?\n",
    "Simple random sampling can over-represent high-volume days. The per-day window-based sampling step aims to keep representation more even across days.\n",
    "\n",
    "### Key constraints\n",
    "- **Avoid converting large Spark DataFrames to Pandas.** Only convert small samples/aggregations.\n",
    "\n",
    "### Inputs\n",
    "- Parquet path: `s3a://okdp/examples/data/raw/tripdata/yellow/`\n",
    "- Credentials from mounted files:\n",
    "  - `/var/run/secrets/examples/s3/S3_ACCESS_KEY`\n",
    "  - `/var/run/secrets/examples/s3/S3_SECRET_KEY`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 1. Setup and Imports\n",
    "\n",
    "Imports used in this notebook:\n",
    "- **SparkSession / functions / Window**: core Spark APIs\n",
    "- **pandas**: used only for small extracts to enable tabular visualization\n",
    "- **altair**: interactive charts\n",
    "\n",
    "Notes:\n",
    "- Keep visualization datasets small to avoid driver memory pressure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "alt.renderers.enable('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11f8b5-8604-4fc2-901b-1d854a999fb5",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 2. Initialize Spark Session\n",
    "\n",
    "This SparkSession is configured for **MinIO/S3A** access:\n",
    "- `spark.hadoop.fs.s3a.endpoint`: MinIO endpoint URL\n",
    "- `SimpleAWSCredentialsProvider`: uses explicit access/secret key values\n",
    "- `path.style.access=true`: important for many MinIO setups\n",
    "\n",
    "If you move to production, you typically:\n",
    "- use IAM roles / workload identity instead of static keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad33456-57cd-4dad-badc-f9efb0919e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "s3AccessKey = Path(\"/var/run/secrets/examples/s3/S3_ACCESS_KEY\").read_text().strip()\n",
    "s3SecretKey = Path(\"/var/run/secrets/examples/s3/S3_SECRET_KEY\").read_text().strip()\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"NYC Tripdata ‚Äî PySpark\")\n",
    "    .config(\"spark.executor.memory\", \"2000M\")\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.executor.instances\", \"1\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"https://minio-default.okdp.sandbox:443\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", s3AccessKey)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", s3SecretKey)\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-data",
   "metadata": {},
   "source": [
    "## üì¶ 2. Load Yellow dataset (Parquet) from S3A\n",
    "\n",
    "We load the Parquet dataset into a Spark DataFrame.\n",
    "\n",
    "Tips:\n",
    "- `printSchema()` is a fast way to validate expected columns and types.\n",
    "- `limit(...).toPandas()` is safe only for small previews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "read-parquet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(f\"s3a://okdp/examples/data/raw/tripdata/yellow/\")\n",
    "df.printSchema()\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "describe-table",
   "metadata": {},
   "source": [
    "## üß± 3. Inspect Table Schema (DESCRIBE TABLE equivalent)\n",
    "\n",
    "This section creates a temporary SQL view and runs `DESCRIBE TABLE`.\n",
    "It is useful when you want a SQL-native workflow (similar to Trino/Hive).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "describe-sql",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"yellow\")\n",
    "spark.sql(\"DESCRIBE TABLE yellow\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sample-balanced",
   "metadata": {},
   "source": [
    "## üîç 4. Query a Random Sample (Daily Balanced Sampling)\n",
    "\n",
    "Goal: build a small dataset that is still representative across time.\n",
    "\n",
    "Approach:\n",
    "- Filter to months **2025-01..2025-03**\n",
    "- Use a window partitioned by pickup date\n",
    "- Assign a random order and take up to **100 trips per day**\n",
    "- Cap at **3000 rows** to keep downstream operations (Pandas/Altair) lightweight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-balanced-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = [\"2025-01\", \"2025-02\", \"2025-03\"]\n",
    "\n",
    "# Filter to Q1 2025\n",
    "base = df.where(F.col(\"month\").isin(months))\n",
    "\n",
    "# Balanced random sample: 100 rows per day\n",
    "w = Window.partitionBy(F.to_date(\"tpep_pickup_datetime\")).orderBy(F.rand())\n",
    "\n",
    "sample = (\n",
    "    base\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .where(F.col(\"rn\") <= 100)\n",
    "    .limit(3000)\n",
    ")\n",
    "\n",
    "sample.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature-engineering",
   "metadata": {},
   "source": [
    "## üïí 5. Time-Based Feature Engineering\n",
    "\n",
    "We derive time features from pickup datetime:\n",
    "- `hour`: hour of day (0‚Äì23)\n",
    "- `day`: weekday name (Monday..Sunday)\n",
    "\n",
    "These features help analyze demand patterns and seasonality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-time-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_fe = (\n",
    "    sample\n",
    "    .withColumn(\"hour\", F.hour(\"tpep_pickup_datetime\"))\n",
    "    .withColumn(\"day\", F.date_format(\"tpep_pickup_datetime\", \"EEEE\"))\n",
    ")\n",
    "\n",
    "sample_fe.select(\"tpep_pickup_datetime\", \"hour\", \"day\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "day-counts",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_fe.groupBy(\"day\").count().orderBy(F.desc(\"count\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dq",
   "metadata": {},
   "source": [
    "## üßπ 6. Data Quality Check\n",
    "\n",
    "We inspect records where `passenger_count == 0`.\n",
    "Depending on your downstream needs, these may be:\n",
    "- data errors\n",
    "- test trips\n",
    "- unusual edge cases\n",
    "\n",
    "Then we filter to `passenger_count > 0` for cleaner EDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dq-describe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = sample_fe.where(F.col(\"passenger_count\") == 0)\n",
    "\n",
    "bad.select(\"trip_distance\", \"fare_amount\", \"total_amount\").describe().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filter-passengers",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_clean = sample_fe.where(F.col(\"passenger_count\") > 0)\n",
    "sample_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scatter",
   "metadata": {},
   "source": [
    "## üìä 7. Visualize Fare vs Distance (Altair)\n",
    "\n",
    "Altair works with in-memory data, so we convert a small Spark subset to Pandas.\n",
    "\n",
    "Visualization goal:\n",
    "- Explore the relationship between `trip_distance` and `fare_amount`\n",
    "- Use interactive highlighting by `passenger_count`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "to-pandas-scatter",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = sample_clean.select(\n",
    "    \"tpep_pickup_datetime\", \"trip_distance\", \"fare_amount\", \"passenger_count\"\n",
    ").toPandas()\n",
    "\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altair-scatter",
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight = alt.selection_point(fields=['passenger_count'], bind='legend')\n",
    "\n",
    "chart = (\n",
    "    alt.Chart(pdf)\n",
    "    .mark_circle(size=40)\n",
    "    .encode(\n",
    "        x='trip_distance:Q',\n",
    "        y='fare_amount:Q',\n",
    "        color=alt.condition(\n",
    "            highlight,\n",
    "            alt.Color('passenger_count:O', scale=alt.Scale(scheme='tableau10')),\n",
    "            alt.value('lightgray')\n",
    "        ),\n",
    "        tooltip=['tpep_pickup_datetime', 'trip_distance', 'fare_amount', 'passenger_count']\n",
    "    )\n",
    "    .add_params(highlight)\n",
    "    .properties(title='NYC Yellow Taxi ‚Äî Interactive Highlight by Passenger Count')\n",
    ")\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly",
   "metadata": {},
   "source": [
    "## ‚è∞ 8. Trips by Hour of Day\n",
    "\n",
    "We aggregate trips by `hour` to understand hourly demand patterns.\n",
    "The output is kept small and then visualized with Altair.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-agg",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly = (\n",
    "    sample_clean.groupBy(\"hour\")\n",
    "    .agg(F.count(\"*\").alias(\"trip_count\"))\n",
    "    .orderBy(\"hour\")\n",
    ")\n",
    "\n",
    "hourly.show(24, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_pdf = hourly.toPandas()\n",
    "\n",
    "alt.Chart(hourly_pdf).mark_bar().encode(\n",
    "    x=alt.X('hour:O', title='Hour of Day'),\n",
    "    y=alt.Y('trip_count:Q', title='Number of Trips'),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('hour:O', title='Hour of Day'),\n",
    "        alt.Tooltip('trip_count:Q', title='Trips')\n",
    "    ]\n",
    ").properties(\n",
    "    title='NYC Trips by Hour of Day'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "avg-fare-dow",
   "metadata": {},
   "source": [
    "## üìÖ 9. Average Fare by Day of Week\n",
    "\n",
    "We compute the average `fare_amount` per weekday.\n",
    "Because weekday names are strings, we define a custom order for clean chart sorting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dow-agg",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = (\n",
    "    sample_clean.groupBy(\"day\")\n",
    "    .agg(F.avg(\"fare_amount\").alias(\"fare_amount\"))\n",
    ")\n",
    "\n",
    "daily.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dow-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_pdf = daily.toPandas()\n",
    "order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "alt.Chart(daily_pdf).mark_bar().encode(\n",
    "    x=alt.X('day:N', sort=order, title='Day of Week'),\n",
    "    y=alt.Y('fare_amount:Q', title='Average Fare ($)'),\n",
    "    tooltip=[\n",
    "        alt.Tooltip('day:N', title='Day of Week'),\n",
    "        alt.Tooltip('fare_amount:Q', title='Average Fare ($)', format='.2f')\n",
    "    ]\n",
    ").properties(\n",
    "    title='Average NYC Taxi Fare by Day of Week'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bernoulli",
   "metadata": {},
   "source": [
    "## üéØ 10. Random Sample (~1%) for Visualization (Spark equivalent)\n",
    "\n",
    "Some SQL engines expose sampling like `TABLESAMPLE BERNOULLI (1)`.\n",
    "In Spark, the closest equivalent is:\n",
    "- `df.sample(withReplacement=False, fraction=0.01, seed=...)`\n",
    "\n",
    "We still apply a `.limit(3000)` to keep the Pandas conversion safe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark-sample-1pct",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_sample = (\n",
    "    df.where(F.col(\"month\").isin(months))\n",
    "          .select(\"trip_distance\", \"fare_amount\", \"total_amount\", \"passenger_count\")\n",
    "          .sample(withReplacement=False, fraction=0.01, seed=42)\n",
    "          .limit(3000)\n",
    ")\n",
    "\n",
    "viz_pdf = viz_sample.toPandas()\n",
    "viz_pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-sample-scatter",
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight = alt.selection_point(fields=['passenger_count'], bind='legend')\n",
    "\n",
    "chart2 = (\n",
    "    alt.Chart(viz_pdf)\n",
    "    .mark_circle(size=40, opacity=0.7)\n",
    "    .encode(\n",
    "        x='trip_distance:Q',\n",
    "        y='fare_amount:Q',\n",
    "        color=alt.condition(\n",
    "            highlight,\n",
    "            alt.Color('passenger_count:O', scale=alt.Scale(scheme='tableau10')),\n",
    "            alt.value('lightgray')\n",
    "        ),\n",
    "        tooltip=['trip_distance', 'fare_amount', 'total_amount', 'passenger_count']\n",
    "    )\n",
    "    .add_params(highlight)\n",
    "    .properties(title='NYC Yellow Taxi ‚Äî Fare vs Distance (Interactive Sample Highlight)')\n",
    ")\n",
    "\n",
    "chart2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "top-pairs",
   "metadata": {},
   "source": [
    "## üöï 11. Top Pickup‚ÄìDropoff Pairs (March 2025)\n",
    "\n",
    "We compute the most frequent pickup/dropoff location pairs for March 2025.\n",
    "\n",
    "Note:\n",
    "- Some datasets use lowercase column names (`pulocationid`, `dolocationid`).\n",
    "- If you get an AnalysisException, adjust column names accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top-pairs-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with standard NYC schema (camel-case). If it fails, rename to lowercase equivalents.\n",
    "top_pairs = (\n",
    "    df.where(F.col(\"month\") == \"2025-03\")\n",
    "          .groupBy(\"PULocationID\", \"DOLocationID\")\n",
    "          .agg(F.count(\"*\").alias(\"trips\"))\n",
    "          .orderBy(F.desc(\"trips\"))\n",
    "          .limit(20)\n",
    ")\n",
    "\n",
    "top_pairs.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## üßæ 12. Statistical Summary\n",
    "\n",
    "`describe()` provides quick descriptive statistics for numeric columns.\n",
    "We convert to Pandas for easier display.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spark-describe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_clean.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "param",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 13. Parameterized Query Example (Spark)\n",
    "\n",
    "Example of using a variable to parameterize a filter.\n",
    "This pattern is useful for building reusable notebooks and jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "param-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "month = \"2025-03\"\n",
    "df.where(F.col(\"month\") == month).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a30a61-da1a-488c-b628-c104d35443bd",
   "metadata": {},
   "source": [
    "## üßπ 14. Stop Spark session\n",
    "\n",
    "Stop the Spark session to free CPU/memory and release any open connections (e.g., to S3/MinIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b639788-0328-4bbb-9e50-d09d30b2d823",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "end",
   "metadata": {},
   "source": [
    "## ‚úÖ 15. Summary\n",
    "- Read Parquet data from `s3a://okdp/...` with Spark\n",
    "- Per-day balanced random sampling using window functions\n",
    "- Feature engineering (hour/day)\n",
    "- Aggregations + Altair visualizations (via small Pandas extracts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dbd64d-b6eb-4051-affc-ba9b072dcf6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
